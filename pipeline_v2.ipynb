{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e5e27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 2.11.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.29.3 which is incompatible.\n",
      "kfp-pipeline-spec 0.6.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.29.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q kfp google-cloud google-cloud-bigquery google-cloud-storage google-cloud-aiplatform db_dtypes google_cloud_pipeline_components pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54aab73-aee6-43c5-acf4-215ccfe92cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz as tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174f733b-111f-4903-8edf-c74829e2824d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcloud auth login\n",
    "#!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3774f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "project_id = 'project-mlops9-cm'\n",
    "dataset_id='coches'\n",
    "folder_path = 'data/input'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743018e",
   "metadata": {},
   "source": [
    "**Carga de CSVs a BigQuery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6eabb9-a02b-4ca1-a60e-e48ddd6c4e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 1d19aa84-3e0b-40d8-8c4d-c40e53015448 for selected_features.csv\n",
      "loaded table 16 rows into project-mlops9-cm.coches.selected_features\n",
      "Starting job 09aba5db-6d02-406e-8da0-c0ae1953ed3a for test_coche.csv\n",
      "loaded table 822 rows into project-mlops9-cm.coches.test_coche\n",
      "Starting job 4095c786-627a-4a6e-a1ac-377fdc8906dd for train_coche.csv\n",
      "loaded table 3284 rows into project-mlops9-cm.coches.train_coche\n"
     ]
    }
   ],
   "source": [
    "dataser_ref = client.dataset(dataset_id)\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):  ## para detectar si la ultima cadena del nombre tenga .csv\n",
    "        table_id = os.path.splitext(file_name)[0]\n",
    "        table_ref = dataser_ref.table(table_id)\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format = bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows = 1,\n",
    "            autodetect = True,\n",
    "        )\n",
    "        \n",
    "        #data/pipeline_train_model_input/filename.csv\n",
    "        csv_file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        with open(csv_file_path, 'rb') as source_file:\n",
    "            load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "            \n",
    "        print(f'Starting job {load_job.job_id} for {file_name}')\n",
    "        \n",
    "        load_job.result() # para indicar q tiene q esperar q el job de carga finalice\n",
    "        \n",
    "        destination_table = client.get_table(table_ref)\n",
    "        print(f'loaded table {destination_table.num_rows} rows into {table_ref}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471cd41c-10a1-4742-8fdc-3296da9a9178",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmatos\\AppData\\Local\\Temp\\ipykernel_16224\\2901874434.py:3: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2 import dsl, compiler\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (component, Input, Output, Dataset)\n",
    "from typing import NamedTuple\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06b0bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-bigquery\"])\n",
    "def validate_data(data_table: str,) -> NamedTuple(\"Outputs\",[(\"condition\", str)],):\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    try:\n",
    "        client.get_table(data_table)\n",
    "        condition = \"true\"\n",
    "    except Exception as e:\n",
    "        condition = \"false\"\n",
    "\n",
    "    return (condition,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dc06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def error_op(msg: str):\n",
    "    raise(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba757aef-309e-4ee9-b76e-ef81e0e06ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cmatos\\AppData\\Local\\anaconda3\\envs\\env-mlops\\lib\\site-packages\\kfp\\dsl\\component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-bigquery\",\n",
    "                               \"google-cloud-bigquery-storage\",\n",
    "                               \"db_dtypes\",\n",
    "                               \"numpy\",\n",
    "                               \"sckikit-learn\",\n",
    "                               \"pyarrow\",\n",
    "                               \"pandas_gbq\"])\n",
    "def process_data(project: str, \n",
    "                 data_table: str,\n",
    "                 dataset: Output[Dataset]):\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pyarrow.parquet as pq\n",
    "    \n",
    "    \n",
    "    client = bigquery.Client(project)\n",
    "    \n",
    "    data = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "    '''.format(dsource_table = data_table)\n",
    "    ).to_dataframe()\n",
    "        \n",
    "    data[\"running\"] = data[\"running\"].astype(str)\n",
    "    data[\"running\"] = data[\"running\"].str.replace(\" km\",\"\")\n",
    "    data[\"running\"] = data[\"running\"].str.replace(\" miles\",\"\")\n",
    "    data[\"running\"] = data[\"running\"].astype(float)\n",
    "    data[\"running\"]\n",
    "\n",
    "    # Variables temporales\n",
    "    TEMPORAL_VARS = ['year']\n",
    "    REF_VAR = datetime.now().year\n",
    "    data[TEMPORAL_VARS] = REF_VAR - data[TEMPORAL_VARS]\n",
    "\n",
    "    data.to_parquet(f'{dataset.path}.parquet', engine='pyarrow', index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74081afd-6a00-47a9-9396-21d9fde9226e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-bigquery\",\n",
    "                               \"google-cloud-bigquery-storage\",\n",
    "                               \"db_dtypes\",\n",
    "                               \"pyarrow\",\n",
    "                               \"joblib\",\n",
    "                               \"sckikit-learn\",\n",
    "                               \"numpy\"])\n",
    "def train_model(project: str, \n",
    "                features_table: str,\n",
    "                inputd: Input[Dataset],\n",
    "                X_train_t: Output[Dataset]):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import Lasso\n",
    "    \n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from feature_engine.transformation import YeoJohnsonTransformer\n",
    "    from feature_engine.encoding import OrdinalEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import joblib\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    data =pd.read_parquet(f'{inputd.path}'.parquet)\n",
    "    #Y_train = client.query(\n",
    "    #'''SELECT * FROM `{dsource_table}`\n",
    "    #'''.format(dsource_table = source_y_train_table)\n",
    "    #).to_dataframe()\n",
    "\n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "    '''.format(dsource_table = features_table)\n",
    "    ).to_dataframe()\n",
    "\n",
    "    \n",
    "    features = features['string_field_0'].tolist()\n",
    "    \n",
    "    # Separamos dataset en train y test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(['price'], axis=1), # predictive variables\n",
    "        data['price'], # target\n",
    "        test_size=0.1, # portion of dataset to allocate to test set\n",
    "        random_state=0, # we are setting the seed here\n",
    "    )\n",
    "    \n",
    "    # Transformación de variable target\n",
    "    y_train = np.log(y_train)\n",
    "    y_test = np.log(y_test)\n",
    "    \n",
    "    # Variables seleccionadas del proceso feature selection\n",
    "    FEATURES = features\n",
    "\n",
    "    X_train = X_train[FEATURES]\n",
    "    X_test = X_test[FEATURES]\n",
    "    \n",
    "    # Variables para tranformaciion yeo \n",
    "    NUMERICALS_YEO_VARS = ['running', 'motor_volume', 'year']\n",
    "\n",
    "    # Variables categoricas para codificar\n",
    "    CATEGORICAL_VARS = ['model', 'motor_type', 'color', 'type', 'status']\n",
    "\n",
    "\n",
    "    X_train[CATEGORICAL_VARS] = X_train[CATEGORICAL_VARS].astype('category')\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "    # VARIABLE TRANSFORMATION\n",
    "     ('yeojohnson', YeoJohnsonTransformer(variables=NUMERICALS_YEO_VARS)),\n",
    "     #('log', LogTransformer(variables=NUMERICALS_YEO_VARS)),\n",
    "        \n",
    "     # CATEGORICAL ENCODING\n",
    "    ('categorical_encoder', OrdinalEncoder(\n",
    "        encoding_method='ordered', variables=CATEGORICAL_VARS)),\n",
    "        \n",
    "    # ESCALAMIENTO\n",
    "    ('scaler', MinMaxScaler()),\n",
    "\n",
    "    # MODELO\n",
    "    ('Lasso', Lasso(alpha=0.001, random_state=0)),\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    pipeline_file_name = '/data/model/precio_coches_pipeline.joblib'\n",
    "    joblib.dump(pipeline, pipeline_file_name)\n",
    "\n",
    "    \n",
    "    # uplead to cs}\n",
    "    storage_client = storage.Client(project = project)\n",
    "    bucket_name = \"cmatos_bk\"\n",
    "    destination_blob_name =\"model/precio_coches_pipeline.joblib\"\n",
    "    \n",
    "    X_train.to_parquet(f'{X_train_t.path}.parquet', engine='pyarrow', index=False)\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(pipeline_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07afcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\",\n",
    "        \"pandas-gbq\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pytz\"\n",
    "    ],\n",
    ")\n",
    "def prediction(\n",
    "    project: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "    input_X_train: Input[Dataset],\n",
    "):  \n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.auth import default\n",
    "    import pandas_gbq\n",
    "    from google.cloud import storage\n",
    "    from joblib import load\n",
    "    from io import BytesIO\n",
    "    from pytz import timezone\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    from datetime import datetime\n",
    "\n",
    "    \n",
    "    X_train =pd.read_parquet(f'{input_X_train.path}'.parquet)\n",
    "\n",
    "    TZ = timezone('America/Lima')\n",
    "    FORMAT_DATE = \"%Y-%m-%d\"\n",
    "    \n",
    "    # Cliente BigQuery\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "\n",
    "    def generate_datetime_created():\n",
    "        return datetime.now()\n",
    "    \n",
    "    def generate_date_created():\n",
    "        return datetime.now(TZ).date().strftime(FORMAT_DATE)\n",
    "    \n",
    "    \n",
    "    def load_model_from_gcs(path_model):\n",
    "        # Inicializar el cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtener el nombre del bucket y la ruta del objeto\n",
    "        bucket_name, blob_name = path_model.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "        # Obtener el objeto desde Cloud Storage\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        model_bytes = blob.download_as_string()\n",
    "\n",
    "        # Cargar el modelo desde los bytes obtenidos\n",
    "        model = load(BytesIO(model_bytes))\n",
    "\n",
    "        return model\n",
    "\n",
    "    model = load_model_from_gcs(path_model)\n",
    "\n",
    "    # Realizar la predicción\n",
    "    predictions = model.predict(X_train)\n",
    "    predicciones_sescalar = np.exp(predictions)\n",
    "    predictions = pd.DataFrame(predicciones_sescalar, columns=['prediction'])\n",
    "\n",
    "    # Obtener el user_id de la sesión actual en BigQuery\n",
    "    user_id = client.query(\"SELECT SESSION_USER()\").to_dataframe().iloc[0, 0]\n",
    "\n",
    "    # Agregar campos de auditoría\n",
    "    start_time = generate_datetime_created()\n",
    "    execute_date = generate_date_created()\n",
    "    \n",
    "    predictions['creation_user'] = user_id\n",
    "    predictions['process_date'] = datetime.strptime(execute_date, '%Y-%m-%d')\n",
    "    predictions['process_date'] = pd.to_datetime(predictions['process_date']).dt.date\n",
    "    predictions['load_date'] = pd.to_datetime(start_time)\n",
    "    \n",
    "    # Guardar el resultado en BigQuery \n",
    "    pandas_gbq.to_gbq(predictions , table_id, if_exists='append', project_id=project)\n",
    "\n",
    "    print(\"Predicción generada y guardada en BigQuery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c5fe815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmatos\\AppData\\Local\\Temp\\ipykernel_16224\\673917678.py:23: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(\n",
      "C:\\Users\\cmatos\\AppData\\Local\\Temp\\ipykernel_16224\\673917678.py:30: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@kfp.dsl.pipeline(name=\"pipeline_m5\",\n",
    "                  description=\"intro\",\n",
    "                  pipeline_root=\"gs://cmatos_bk/demo\")\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    data_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    \n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=[\"jchristian.mp@gmail.com\"])\n",
    "    notify_email_task.set_display_name('Notification Email')\n",
    "    \n",
    "    with dsl.ExitHandler(notify_email_task, name=\"Execute pipeline prediction\"):\n",
    "\n",
    "        validate_tables_job = validate_data(\n",
    "            data_table = data_table\n",
    "        )\n",
    "        validate_tables_job.set_display_name('Validate Data')\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"false\",\n",
    "            name=\"no-execute\",\n",
    "        ):\n",
    "            error_op(msg=\"No se logro validar las tablas de ingesta.\")\n",
    "\n",
    "\n",
    "        with dsl.Condition(\n",
    "            validate_tables_job.outputs['condition']==\"true\",\n",
    "            name=\"execute\",\n",
    "        ):\n",
    "  \n",
    "            process_dataset = process_data(\n",
    "                project=project,\n",
    "                data_table=data_table                \n",
    "            )\n",
    "            process_dataset.set_display_name(\"Process_Data\")\n",
    "\n",
    "            train = train_model(\n",
    "                project=project,\n",
    "                features_table=features_table,\n",
    "                inputd=process_dataset.output\n",
    "            ).after(process_dataset)\n",
    "            train.set_display_name(\"Train_Model\")\n",
    "\n",
    "            prediction_model = prediction(\n",
    "                project = project,\n",
    "                table_id = table_id,\n",
    "                path_model = path_model,\n",
    "                input_X_train=train.output\n",
    "            ).after(train)\n",
    "            prediction_model.set_display_name(\"Prediction_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad12f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipelinemodel_m5.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41d3b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo pipelinemodel_m5.json subido a demo/pipelinemodel_m5.json en el bucket cmatos_bk.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"Archivo {source_file_name} subido a {destination_blob_name} en el bucket {bucket_name}.\")\n",
    "\n",
    "# Define las variables\n",
    "bucket_name = \"cmatos_bk\"\n",
    "destination_blob_name = \"demo/pipelinemodel_m5.json\"\n",
    "pipeline_file = \"pipelinemodel_m5.json\"\n",
    "# Llamar a la función para subir el archivo\n",
    "upload_to_gcs(bucket_name, pipeline_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2045716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"project-mlops9-cm\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98a7cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/129100651954/locations/us-central1/pipelineJobs/pipeline-m5-20250115015233\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/129100651954/locations/us-central1/pipelineJobs/pipeline-m5-20250115015233')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-m5-20250115015233?project=129100651954\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline coches\",\n",
    "    template_path=\"gs://cmatos_bk/demo/pipelinemodel_m5.json\",\n",
    "    #job_id=\"mlops72\",\n",
    "    enable_caching=False,\n",
    "    project=\"project-mlops9-cm\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"project-mlops9-cm\", \n",
    "                      \"data_table\": \"project-mlops9-cm.coches.train_coche\",\n",
    "                      \"features_table\": \"project-mlops9-cm.coches.selected_features\",\n",
    "                      \"table_id\": \"project-mlops9-cm.coches.predictions\",\n",
    "                      \"path_model\": \"gs://cmatos_bk/model/precio_coches_pipeline.joblib\"\n",
    "                     }\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"<<account>>@project-mlops9-cm.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef5b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "dev",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "env-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
